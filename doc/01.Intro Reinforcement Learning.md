## Introduction to Reinforcement Learning [Lecture Video](https://www.youtube.com/watch?v=2pWv7GOvuf0&t=3338)

---

- **Reinforcement Learning Paradigm**
- there is no supervisor, only reward signal
- feedback may not be instantaneous
- time really matters (sequential non i.i.d data)
- agent's actions affect the subsequent data it receives

---

- **Rewards**
- reward Rt is a scalar feedback signal
- indicates how well agent is doing at step t
- agent's job is to maximise cumulative reward
- **Reward Hypothesis** all goals can be described by the maximisaion of expected cumulative reward
- ultimately we are after picking actions, thus, there must always exist a conversion between different objectives

---

- **Observation, Action, Reward**
- at each step t the agent
- executes action At
- receives observation Ot
- receives reward Rt
- agent influences the environment through the action it makes
- history $Ht=A_1,O_1,R_1..A_t,O_t,R_t$
- history determines what happens next

---

- **State**
- state is the information used to determine what happens next
- state is a function of history $S_t=f(H_t)$
- different definitions of state : observation that we see, agent state, environment state

---

- **Markov Property**
- $P[S_t+_1 | S] = P[S_t+_1 | S_1,...S_t]$
- _the future is independent of the past given the present_
- state is sufficient statistic of the future
- ultimately we are after making decision about actions for the future
- the state representation defines what happens next - our job is to build a state representation that is actually useful for predicting what is about to happen next

---

- **Environment Observability**
- in Fully Observable Environments agent directly observes environment state
- agent = environment = information state $O_t=S_t^a=S_t^e$
- this is **Markov Decision Process (MDP)**
- Partial observability where agent indirectly observes environment
- this is **Partially observable Markov Decision Process (POMDP)**
- agent must construct its own state representation
- state can be constructed based on fully history or based on beliefs (bayesian approach)

---

- **RL Agent components**
- policy : agent's behaviour function
- value function : how good is each state and action
- model : agent's representation of the environment
- this is the superset of components that may be used by an agent

<br>

- **policy** is a map from state to action
- deterministic policy $a=\pi(s)$
- stochastic policy $\pi(a|s)=P[A=a|S=s]$

<br>

- **value function** is a prediction of future reward
- used to evaluate the goodness/badness of states
- $v_\pi(s)=E_\pi[R_t+\gamma R_t+_1 + \gamma^2 R_t+_2 + ... |S=s]$
- value function depends on the way the agent is behaving, i.e. on the policy that follows - which explains the $\pi$ subscript
- the horizon of the reward is determined by the discount factor $\gamma$
- risk is already accounted for since we are calculating the expected reward

<br>

- **model** predicts what the environment will do next
- _state transition_ model used to predict the next state : $P_{ss'}^a=P[S'=s'|S=s,A=a]$
- _rewards_ transition model : $R_s^a=E[R|S=s,A=a]$
- model is optional in several cases and this is what we call model free methods

---

- **Agents Taxonomy**
- we taxonomize the rl agents based on which of the key components are present
- value based - if contains a value function - in such cases, policy is implicit (i.e. always pick the best value function) - agent stores the value
- policy based - data structure to maintain the optimal policy without ever explicitly calculate the value function - agent stores the policy
- actor critic - is the combination of the above two, i.e. an agent that stores and operates based on both
- **Agents Taxonomy based on model**
- model free - agent does not explicitly try to represent the environment, instead its decisions are based on value function or policy, without explicitly try to model how the environment works
- model based - we build a model to understand the environment and use this in conjuction with policy and value function

---

**Reward vs Return vs Value Function**

- Reward: A reward is a numerical signal provided by the environment to an agent at each step of an MDP. It represents the immediate desirability or quality of a particular state-action pair. In other words, it indicates how much the agent values the outcome of taking a specific action in a specific state. The reward is typically denoted by the symbol "R" and can be positive, negative, or zero.

- Return: The return is the cumulative total of rewards obtained from a specific state onward until the end of an episode (or the end of the task). It represents the notion of long-term desirability or value of being in a particular state and taking a particular action. The return is often denoted by the symbol "G" and can be defined in different ways, such as the sum of discounted rewards or the sum of rewards until a terminal state is reached.

1Value function: A value function estimates the expected return or value of being in a particular state (or state-action pair) under a given policy. It quantifies the desirability or quality of states based on the expected future rewards that can be obtained from those states. There are two main types of value functions in MDPs:

- State value function (V(s)): It represents the expected return from being in a particular state "s" under a given policy. It quantifies the long-term value or desirability of a state.

- Action value function (Q(s, a)): It represents the expected return from taking a particular action "a" in a particular state "s" under a given policy. It quantifies the long-term value or desirability of taking a specific action in a specific state.

Both value functions are defined recursively based on the Bellman equation, which relates the value of a state or state-action pair to the values of its successor states or successor state-action pairs.

In summary, the reward is the immediate signal indicating the quality of a state-action pair, the return is the cumulative total of rewards obtained from a state until the end of an episode, and the value function estimates the long-term desirability or value of being in a state or taking an action under a given policy.

- **Sequential Decision Making**
- Reinforcement Learning : environment is _initially unknown_, agent interacts with the environment and improves its policy
- RL is like a trial and error approach, the agent shoud discover a good policy in an unknown environment, without losing too much reward along the way
- this is where the **exploration vs exploitation** dilemma comes into play
- Planning : a model of the environment is _known_ , the agent performs computations with its model and improves its policy
- in planning, the agent has perfect understanding of the environment and can essentially "look-ahead"
- prediction (in rl lingo) evaluate the future given a policy
- control (in rl lingo) optimise the future by finding the best policy
- oftentimes, we first need to solve the prediction problem (i.e. evaluate potential policies) before selecting the optimal one

## Model Free Prediction

- So far we have seen methods to solve MRP | MDP provided full knowledge over the dynamic governing our system.
- Model Free prediciton is about solving the MRP without prior knowledge of its dynamics
- Core approaches include Monte-Carlo Learning and Temporal Differencing Learning

**Monte Carlo (MC)**:

- Monte Carlo methods estimate value functions based on complete episodes of interaction with the environment.
- These methods sample sequences of states, actions, and rewards until the end of an episode, and then use the obtained returns (cumulative rewards) to update value estimates.
- Monte Carlo methods converge to the solution with the minimum mean-squared error (MSE), meaning they converge to the value function that minimizes the average squared difference between the estimated and true values.
- By averaging over multiple episodes, Monte Carlo methods reduce the effects of initial conditions and fluctuations, leading to more accurate value estimates as the number of episodes increases.

**Temporal Difference (TD)**:

- Temporal Difference methods update value estimates incrementally after each time step, based on the current state, immediate reward, and the estimated value of the next state.
- TD methods combine ideas from both Monte Carlo and dynamic programming approaches, allowing for online learning and updating value estimates without requiring complete episodes.
- Unlike Monte Carlo methods, TD methods converge to the solution of a maximum likelihood Markov model, which means they converge to the value function that is closest to the true underlying Markov model of the environment.
- TD methods make use of bootstrapping, where the value of the next state is estimated based on the current value function, incorporating predictions of future states into the update process.

**MD & TD Convergence**:

Monte Carlo methods converge to the solution with the minimum mean-squared error (MSE), which means they converge to the value function that minimizes the average squared difference between the estimated and true values.
The MSE is a measure of how close the estimated values are to the true values. By minimizing the MSE, MC methods aim to find the value function that provides the most accurate representation of the true underlying value function.
Convergence to the minimum MSE occurs as the number of episodes increases. By averaging over multiple episodes, MC methods reduce the effects of initial conditions and random fluctuations, leading to more accurate value estimates.

Temporal Difference methods converge to the solution of a maximum likelihood Markov model. This means they converge to the value function that is closest to the true underlying Markov model of the environment.
The maximum likelihood Markov model represents the most likely transition dynamics and rewards in the environment. TD methods aim to approximate this model by iteratively updating value estimates based on observed state transitions and immediate rewards.
By incorporating predictions of future states into the update process, TD methods gradually refine the value estimates, aligning them with the expected behavior of the environment according to the maximum likelihood model.
